{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayakpaul/Generalized-ODIN-TF/blob/main/Generalized_ODIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFwWgoioRXy9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCQ1S81KIbaN",
    "outputId": "5a9b4b37-56fd-4665-b754-a0e267a094d0"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import urllib\n",
    "# from getpass import getpass\n",
    "\n",
    "# user = input('User name: ')\n",
    "# password = getpass('Password: ')\n",
    "# password = urllib.parse.quote(password)\n",
    "# repo_address = input('Repo Address: ')\n",
    "# branch_name = input('Branch name: ')\n",
    "\n",
    "# cmd_string = 'git clone https://{}:{}@github.com/{}.git -b {}'.format(\n",
    "#     user, password, repo_address, branch_name\n",
    "# )\n",
    "\n",
    "# os.system(cmd_string)\n",
    "# cmd_string, password = \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/home/jupyter/.local/bin/gdown --id 1ap08uSszQFKu972dv0B8Bo7FjI6IMKnD\n",
    "# !tar xf initial_model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QtwAr7vgIpS5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Generalized-ODIN-TF\")\n",
    "\n",
    "from scripts import resnet20_odin, resnet20\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWpj-WTpRZvS"
   },
   "source": [
    "## Load CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6mPJL_5JDsO",
    "outputId": "500c0fc7-dd46-45ae-9245-c2ae4ef6111a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 50000\n",
      "Total test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Total training examples: {len(x_train)}\")\n",
    "print(f\"Total test examples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMIkjg6_RbYc"
   },
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5A1ZdFW6J4_R"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "START_LR = 0.1\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQQLiGNRdvx"
   },
   "source": [
    "## Prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GqNtOj8yKO_X"
   },
   "outputs": [],
   "source": [
    "# Augmentation pipeline\n",
    "simple_aug = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now, map the augmentation pipeline to our training dataset\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y: (simple_aug(x), y), num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JOCwKSQRfvl"
   },
   "source": [
    "## Utility function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ssNNrdG7K2bw"
   },
   "outputs": [],
   "source": [
    "def get_rn_model(arch, num_classes=10):\n",
    "    n = 2\n",
    "    depth = n * 9 + 2\n",
    "    n_blocks = ((depth - 2) // 9) - 1\n",
    "\n",
    "    # The input tensor\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1)(\n",
    "        inputs\n",
    "    )\n",
    "\n",
    "    # The Stem Convolution Group\n",
    "    x = arch.stem(x)\n",
    "\n",
    "    # The learner\n",
    "    x = arch.learner(x, n_blocks)\n",
    "\n",
    "    # The Classifier for 10 classes\n",
    "    outputs = arch.classifier(x, num_classes)\n",
    "\n",
    "    # Instantiate the Model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# First serialize an initial ResNet20 model for reproducibility\n",
    "# initial_model = get_rn_model(resnet20)\n",
    "# initial_model.save(\"initial_model\")\n",
    "initial_model = tf.keras.models.load_model(\"initial_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set the initial model weights of our ODIN model\n",
    "odin_rn_model = get_rn_model(resnet20_odin)\n",
    "\n",
    "for rn20_layer, rn20_odin_layer in zip(initial_model.layers[:-2], \n",
    "                                       odin_rn_model.layers[:-6]):\n",
    "    rn20_odin_layer.set_weights(rn20_layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXUFugU3Riph"
   },
   "source": [
    "## Define LR schedule, optimizer, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8a6VIseo7GFP"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < int(EPOCHS * 0.25) - 1:\n",
    "        return START_LR\n",
    "    elif epoch < int(EPOCHS*0.5) -1:\n",
    "        return float(START_LR * 0.1)\n",
    "    elif epoch < int(EPOCHS*0.75) -1:\n",
    "        return float(START_LR * 0.01)\n",
    "    else:\n",
    "        return float(START_LR * 0.001)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iUhtb9ZiM17B"
   },
   "outputs": [],
   "source": [
    "# Optimizer and loss function.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=START_LR, momentum=0.9)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkU9afGCRnaC"
   },
   "source": [
    "## Model training with ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3zhJ1iFPLzp",
    "outputId": "a858aef4-4c17-4b90-cc10-413707d2cbff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 14s 25ms/step - loss: 2.9166 - accuracy: 0.2779 - val_loss: 2.3215 - val_accuracy: 0.3312\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.8878 - accuracy: 0.5003 - val_loss: 2.5191 - val_accuracy: 0.3967\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.4423 - accuracy: 0.6060 - val_loss: 2.1238 - val_accuracy: 0.4748\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.2411 - accuracy: 0.6607 - val_loss: 2.3236 - val_accuracy: 0.4316\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.1440 - accuracy: 0.6885 - val_loss: 1.7209 - val_accuracy: 0.5615\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.0871 - accuracy: 0.7143 - val_loss: 1.6530 - val_accuracy: 0.5478\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.0640 - accuracy: 0.7235 - val_loss: 1.7577 - val_accuracy: 0.5451\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.0292 - accuracy: 0.7363 - val_loss: 1.5193 - val_accuracy: 0.6054\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.0222 - accuracy: 0.7404 - val_loss: 1.2739 - val_accuracy: 0.6824\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.0001 - accuracy: 0.7520 - val_loss: 1.4943 - val_accuracy: 0.6099\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 1.0007 - accuracy: 0.7533 - val_loss: 1.8197 - val_accuracy: 0.5542\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9917 - accuracy: 0.7584 - val_loss: 1.8238 - val_accuracy: 0.5557\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9867 - accuracy: 0.7618 - val_loss: 1.4385 - val_accuracy: 0.6499\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9742 - accuracy: 0.7670 - val_loss: 2.4004 - val_accuracy: 0.4881\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9620 - accuracy: 0.7742 - val_loss: 1.1470 - val_accuracy: 0.7088\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9590 - accuracy: 0.7741 - val_loss: 2.0239 - val_accuracy: 0.5560\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9693 - accuracy: 0.7730 - val_loss: 1.3904 - val_accuracy: 0.6537\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9629 - accuracy: 0.7772 - val_loss: 1.4589 - val_accuracy: 0.6143\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.9533 - accuracy: 0.7828 - val_loss: 1.1538 - val_accuracy: 0.7139\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9444 - accuracy: 0.7822 - val_loss: 1.0852 - val_accuracy: 0.7280\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9570 - accuracy: 0.7852 - val_loss: 1.8967 - val_accuracy: 0.5399\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.9403 - accuracy: 0.7845 - val_loss: 1.0787 - val_accuracy: 0.7467\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9439 - accuracy: 0.7883 - val_loss: 2.0921 - val_accuracy: 0.5185\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9393 - accuracy: 0.7866 - val_loss: 1.2502 - val_accuracy: 0.6867\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9425 - accuracy: 0.7895 - val_loss: 1.7104 - val_accuracy: 0.5882\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9389 - accuracy: 0.7911 - val_loss: 1.3772 - val_accuracy: 0.6667\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.9337 - accuracy: 0.7939 - val_loss: 1.2194 - val_accuracy: 0.6935\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9293 - accuracy: 0.7929 - val_loss: 1.3304 - val_accuracy: 0.6759\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9289 - accuracy: 0.7954 - val_loss: 1.4724 - val_accuracy: 0.6371\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9219 - accuracy: 0.7998 - val_loss: 1.4115 - val_accuracy: 0.6595\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9325 - accuracy: 0.7948 - val_loss: 2.2649 - val_accuracy: 0.4961\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9235 - accuracy: 0.7982 - val_loss: 1.2627 - val_accuracy: 0.7054\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9240 - accuracy: 0.7959 - val_loss: 1.2232 - val_accuracy: 0.7087\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9183 - accuracy: 0.8016 - val_loss: 1.7276 - val_accuracy: 0.6153\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9247 - accuracy: 0.8006 - val_loss: 1.1871 - val_accuracy: 0.7118\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9149 - accuracy: 0.8031 - val_loss: 1.0587 - val_accuracy: 0.7535\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.9182 - accuracy: 0.8018 - val_loss: 1.2840 - val_accuracy: 0.6817\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9221 - accuracy: 0.8009 - val_loss: 1.6142 - val_accuracy: 0.6266\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9226 - accuracy: 0.8026 - val_loss: 1.3921 - val_accuracy: 0.6505\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9178 - accuracy: 0.8013 - val_loss: 1.4432 - val_accuracy: 0.6598\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.9220 - accuracy: 0.8013 - val_loss: 1.4479 - val_accuracy: 0.6462\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9269 - accuracy: 0.8024 - val_loss: 1.2276 - val_accuracy: 0.7043\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9126 - accuracy: 0.8061 - val_loss: 1.5500 - val_accuracy: 0.6455\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9096 - accuracy: 0.8086 - val_loss: 1.8072 - val_accuracy: 0.5635\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9175 - accuracy: 0.8094 - val_loss: 1.7567 - val_accuracy: 0.6083\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.9148 - accuracy: 0.8066 - val_loss: 3.4083 - val_accuracy: 0.4498\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9184 - accuracy: 0.8065 - val_loss: 2.1591 - val_accuracy: 0.5091\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9085 - accuracy: 0.8109 - val_loss: 2.2507 - val_accuracy: 0.5506\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.9144 - accuracy: 0.8078 - val_loss: 1.8021 - val_accuracy: 0.6140\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.8233 - accuracy: 0.8381 - val_loss: 0.7611 - val_accuracy: 0.8568\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.6663 - accuracy: 0.8842 - val_loss: 0.7466 - val_accuracy: 0.8556\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.6201 - accuracy: 0.8929 - val_loss: 0.7656 - val_accuracy: 0.8432\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.5866 - accuracy: 0.8999 - val_loss: 0.7284 - val_accuracy: 0.8507\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.5509 - accuracy: 0.9016 - val_loss: 0.6608 - val_accuracy: 0.8688\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.5263 - accuracy: 0.9067 - val_loss: 0.6426 - val_accuracy: 0.8642\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.5049 - accuracy: 0.9075 - val_loss: 0.6839 - val_accuracy: 0.8522\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4817 - accuracy: 0.9123 - val_loss: 0.7515 - val_accuracy: 0.8370\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4746 - accuracy: 0.9107 - val_loss: 0.6052 - val_accuracy: 0.8709\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4598 - accuracy: 0.9125 - val_loss: 0.6446 - val_accuracy: 0.8599\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4481 - accuracy: 0.9163 - val_loss: 0.6249 - val_accuracy: 0.8655\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4459 - accuracy: 0.9132 - val_loss: 0.7122 - val_accuracy: 0.8349\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4356 - accuracy: 0.9166 - val_loss: 0.7617 - val_accuracy: 0.8267\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4271 - accuracy: 0.9174 - val_loss: 0.5891 - val_accuracy: 0.8713\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4337 - accuracy: 0.9151 - val_loss: 0.6859 - val_accuracy: 0.8450\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4201 - accuracy: 0.9152 - val_loss: 0.6840 - val_accuracy: 0.8447\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4214 - accuracy: 0.9162 - val_loss: 0.6163 - val_accuracy: 0.8607\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4124 - accuracy: 0.9175 - val_loss: 0.6534 - val_accuracy: 0.8512\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4100 - accuracy: 0.9186 - val_loss: 0.6332 - val_accuracy: 0.8525\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4084 - accuracy: 0.9181 - val_loss: 0.5912 - val_accuracy: 0.8630\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4083 - accuracy: 0.9210 - val_loss: 0.6246 - val_accuracy: 0.8602\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4071 - accuracy: 0.9201 - val_loss: 0.5875 - val_accuracy: 0.8627\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4047 - accuracy: 0.9205 - val_loss: 0.7790 - val_accuracy: 0.8129\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4093 - accuracy: 0.9178 - val_loss: 0.7753 - val_accuracy: 0.8174\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4048 - accuracy: 0.9194 - val_loss: 0.6917 - val_accuracy: 0.8382\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4067 - accuracy: 0.9183 - val_loss: 0.7019 - val_accuracy: 0.8354\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4050 - accuracy: 0.9211 - val_loss: 0.7095 - val_accuracy: 0.8319\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3986 - accuracy: 0.9237 - val_loss: 0.7404 - val_accuracy: 0.8342\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4020 - accuracy: 0.9229 - val_loss: 0.7029 - val_accuracy: 0.8393\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3971 - accuracy: 0.9240 - val_loss: 0.7388 - val_accuracy: 0.8296\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4061 - accuracy: 0.9213 - val_loss: 0.7544 - val_accuracy: 0.8250\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.3985 - accuracy: 0.9242 - val_loss: 0.6566 - val_accuracy: 0.8511\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4005 - accuracy: 0.9237 - val_loss: 0.6887 - val_accuracy: 0.8481\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.3902 - accuracy: 0.9281 - val_loss: 0.7615 - val_accuracy: 0.8227\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3869 - accuracy: 0.9286 - val_loss: 0.7023 - val_accuracy: 0.8434\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4027 - accuracy: 0.9220 - val_loss: 0.8256 - val_accuracy: 0.8114\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4053 - accuracy: 0.9236 - val_loss: 0.7015 - val_accuracy: 0.8508\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3966 - accuracy: 0.9254 - val_loss: 0.7571 - val_accuracy: 0.8378\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3951 - accuracy: 0.9264 - val_loss: 0.7253 - val_accuracy: 0.8399\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.3922 - accuracy: 0.9278 - val_loss: 0.7442 - val_accuracy: 0.8348\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.4025 - accuracy: 0.9250 - val_loss: 0.7206 - val_accuracy: 0.8351\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4033 - accuracy: 0.9246 - val_loss: 0.7320 - val_accuracy: 0.8365\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3914 - accuracy: 0.9294 - val_loss: 0.6760 - val_accuracy: 0.8512\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.4018 - accuracy: 0.9257 - val_loss: 0.7350 - val_accuracy: 0.8408\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.3913 - accuracy: 0.9297 - val_loss: 0.6404 - val_accuracy: 0.8583\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3946 - accuracy: 0.9303 - val_loss: 0.6574 - val_accuracy: 0.8533\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3989 - accuracy: 0.9270 - val_loss: 0.8554 - val_accuracy: 0.8151\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3942 - accuracy: 0.9283 - val_loss: 0.7424 - val_accuracy: 0.8427\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3852 - accuracy: 0.9310 - val_loss: 0.7017 - val_accuracy: 0.8441\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.3906 - accuracy: 0.9316 - val_loss: 0.7225 - val_accuracy: 0.8390\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3726 - accuracy: 0.9372 - val_loss: 0.5392 - val_accuracy: 0.8962\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.3134 - accuracy: 0.9597 - val_loss: 0.5238 - val_accuracy: 0.9016\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2966 - accuracy: 0.9648 - val_loss: 0.5226 - val_accuracy: 0.9033\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2841 - accuracy: 0.9689 - val_loss: 0.5142 - val_accuracy: 0.9056\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2783 - accuracy: 0.9692 - val_loss: 0.5345 - val_accuracy: 0.9024\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2735 - accuracy: 0.9712 - val_loss: 0.5321 - val_accuracy: 0.9035\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2674 - accuracy: 0.9730 - val_loss: 0.5604 - val_accuracy: 0.8979\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.2629 - accuracy: 0.9742 - val_loss: 0.5437 - val_accuracy: 0.9035\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2586 - accuracy: 0.9745 - val_loss: 0.5368 - val_accuracy: 0.9012\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.2569 - accuracy: 0.9754 - val_loss: 0.5283 - val_accuracy: 0.9053\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2492 - accuracy: 0.9766 - val_loss: 0.5347 - val_accuracy: 0.9066\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2471 - accuracy: 0.9778 - val_loss: 0.5500 - val_accuracy: 0.9037\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2422 - accuracy: 0.9781 - val_loss: 0.5488 - val_accuracy: 0.9025\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2413 - accuracy: 0.9789 - val_loss: 0.5438 - val_accuracy: 0.9056\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2416 - accuracy: 0.9773 - val_loss: 0.5415 - val_accuracy: 0.9046\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2373 - accuracy: 0.9785 - val_loss: 0.5699 - val_accuracy: 0.8979\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2338 - accuracy: 0.9801 - val_loss: 0.5491 - val_accuracy: 0.9040\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2301 - accuracy: 0.9806 - val_loss: 0.5551 - val_accuracy: 0.9035\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2264 - accuracy: 0.9813 - val_loss: 0.5570 - val_accuracy: 0.9043\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2231 - accuracy: 0.9821 - val_loss: 0.5575 - val_accuracy: 0.9035\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2246 - accuracy: 0.9811 - val_loss: 0.5659 - val_accuracy: 0.9010\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2210 - accuracy: 0.9818 - val_loss: 0.5788 - val_accuracy: 0.9008\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2183 - accuracy: 0.9828 - val_loss: 0.5576 - val_accuracy: 0.9050\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2155 - accuracy: 0.9836 - val_loss: 0.5541 - val_accuracy: 0.9052\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2154 - accuracy: 0.9822 - val_loss: 0.5829 - val_accuracy: 0.9021\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2141 - accuracy: 0.9834 - val_loss: 0.5614 - val_accuracy: 0.9057\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2114 - accuracy: 0.9832 - val_loss: 0.5845 - val_accuracy: 0.9010\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2090 - accuracy: 0.9835 - val_loss: 0.5730 - val_accuracy: 0.9027\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2086 - accuracy: 0.9838 - val_loss: 0.5734 - val_accuracy: 0.9040\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.2025 - accuracy: 0.9862 - val_loss: 0.5754 - val_accuracy: 0.9028\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.2049 - accuracy: 0.9853 - val_loss: 0.5648 - val_accuracy: 0.9063\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.2001 - accuracy: 0.9847 - val_loss: 0.5747 - val_accuracy: 0.9060\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2019 - accuracy: 0.9848 - val_loss: 0.5864 - val_accuracy: 0.9041\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1989 - accuracy: 0.9850 - val_loss: 0.5741 - val_accuracy: 0.9046\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.2005 - accuracy: 0.9843 - val_loss: 0.6097 - val_accuracy: 0.8988\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1979 - accuracy: 0.9854 - val_loss: 0.5941 - val_accuracy: 0.9031\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1944 - accuracy: 0.9853 - val_loss: 0.5874 - val_accuracy: 0.9045\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1958 - accuracy: 0.9844 - val_loss: 0.5746 - val_accuracy: 0.9062\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1923 - accuracy: 0.9860 - val_loss: 0.6044 - val_accuracy: 0.8994\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1955 - accuracy: 0.9842 - val_loss: 0.5730 - val_accuracy: 0.9075\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1883 - accuracy: 0.9869 - val_loss: 0.5917 - val_accuracy: 0.9049\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1889 - accuracy: 0.9860 - val_loss: 0.6044 - val_accuracy: 0.9027\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1894 - accuracy: 0.9862 - val_loss: 0.5923 - val_accuracy: 0.9035\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.1863 - accuracy: 0.9869 - val_loss: 0.6162 - val_accuracy: 0.8985\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.1820 - accuracy: 0.9872 - val_loss: 0.6064 - val_accuracy: 0.9015\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1826 - accuracy: 0.9869 - val_loss: 0.5948 - val_accuracy: 0.9043\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1808 - accuracy: 0.9873 - val_loss: 0.6064 - val_accuracy: 0.8985\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1805 - accuracy: 0.9875 - val_loss: 0.5882 - val_accuracy: 0.9042\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1796 - accuracy: 0.9871 - val_loss: 0.5756 - val_accuracy: 0.9073\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.1818 - accuracy: 0.9860 - val_loss: 0.5802 - val_accuracy: 0.9030\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1740 - accuracy: 0.9890 - val_loss: 0.5785 - val_accuracy: 0.9058\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1708 - accuracy: 0.9903 - val_loss: 0.5774 - val_accuracy: 0.9051\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1720 - accuracy: 0.9886 - val_loss: 0.5750 - val_accuracy: 0.9052\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1707 - accuracy: 0.9897 - val_loss: 0.5769 - val_accuracy: 0.9054\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1676 - accuracy: 0.9908 - val_loss: 0.5741 - val_accuracy: 0.9062\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1677 - accuracy: 0.9907 - val_loss: 0.5785 - val_accuracy: 0.9049\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1669 - accuracy: 0.9912 - val_loss: 0.5734 - val_accuracy: 0.9057\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1688 - accuracy: 0.9903 - val_loss: 0.5729 - val_accuracy: 0.9057\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1651 - accuracy: 0.9914 - val_loss: 0.5752 - val_accuracy: 0.9055\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1651 - accuracy: 0.9910 - val_loss: 0.5744 - val_accuracy: 0.9054\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1656 - accuracy: 0.9918 - val_loss: 0.5730 - val_accuracy: 0.9063\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1661 - accuracy: 0.9916 - val_loss: 0.5746 - val_accuracy: 0.9067\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1648 - accuracy: 0.9917 - val_loss: 0.5779 - val_accuracy: 0.9069\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1654 - accuracy: 0.9912 - val_loss: 0.5747 - val_accuracy: 0.9073\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1642 - accuracy: 0.9918 - val_loss: 0.5755 - val_accuracy: 0.9070\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1602 - accuracy: 0.9935 - val_loss: 0.5779 - val_accuracy: 0.9063\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1614 - accuracy: 0.9925 - val_loss: 0.5765 - val_accuracy: 0.9071\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1609 - accuracy: 0.9929 - val_loss: 0.5793 - val_accuracy: 0.9069\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1627 - accuracy: 0.9926 - val_loss: 0.5808 - val_accuracy: 0.9060\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1633 - accuracy: 0.9918 - val_loss: 0.5828 - val_accuracy: 0.9069\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1621 - accuracy: 0.9924 - val_loss: 0.5773 - val_accuracy: 0.9079\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1620 - accuracy: 0.9923 - val_loss: 0.5796 - val_accuracy: 0.9067\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1616 - accuracy: 0.9924 - val_loss: 0.5825 - val_accuracy: 0.9065\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1620 - accuracy: 0.9925 - val_loss: 0.5789 - val_accuracy: 0.9069\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1609 - accuracy: 0.9932 - val_loss: 0.5792 - val_accuracy: 0.9063\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1595 - accuracy: 0.9934 - val_loss: 0.5791 - val_accuracy: 0.9061\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1613 - accuracy: 0.9926 - val_loss: 0.5794 - val_accuracy: 0.9067\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1591 - accuracy: 0.9930 - val_loss: 0.5830 - val_accuracy: 0.9070\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1615 - accuracy: 0.9924 - val_loss: 0.5825 - val_accuracy: 0.9070\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1595 - accuracy: 0.9933 - val_loss: 0.5850 - val_accuracy: 0.9067\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1601 - accuracy: 0.9933 - val_loss: 0.5865 - val_accuracy: 0.9057\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1600 - accuracy: 0.9921 - val_loss: 0.5895 - val_accuracy: 0.9058\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1591 - accuracy: 0.9938 - val_loss: 0.5923 - val_accuracy: 0.9055\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1598 - accuracy: 0.9927 - val_loss: 0.5919 - val_accuracy: 0.9052\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1552 - accuracy: 0.9940 - val_loss: 0.5924 - val_accuracy: 0.9066\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1579 - accuracy: 0.9932 - val_loss: 0.5879 - val_accuracy: 0.9075\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1598 - accuracy: 0.9921 - val_loss: 0.5924 - val_accuracy: 0.9062\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1589 - accuracy: 0.9933 - val_loss: 0.5846 - val_accuracy: 0.9086\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1575 - accuracy: 0.9931 - val_loss: 0.5883 - val_accuracy: 0.9080\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1576 - accuracy: 0.9937 - val_loss: 0.5824 - val_accuracy: 0.9095\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1558 - accuracy: 0.9938 - val_loss: 0.5892 - val_accuracy: 0.9079\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1605 - accuracy: 0.9926 - val_loss: 0.5860 - val_accuracy: 0.9078\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 0.1579 - accuracy: 0.9936 - val_loss: 0.5922 - val_accuracy: 0.9062\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1581 - accuracy: 0.9926 - val_loss: 0.5955 - val_accuracy: 0.9061\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1558 - accuracy: 0.9936 - val_loss: 0.5925 - val_accuracy: 0.9067\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1564 - accuracy: 0.9940 - val_loss: 0.5908 - val_accuracy: 0.9069\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1553 - accuracy: 0.9936 - val_loss: 0.5909 - val_accuracy: 0.9082\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1561 - accuracy: 0.9937 - val_loss: 0.5931 - val_accuracy: 0.9071\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1562 - accuracy: 0.9935 - val_loss: 0.5923 - val_accuracy: 0.9075\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1529 - accuracy: 0.9941 - val_loss: 0.5922 - val_accuracy: 0.9074\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.1547 - accuracy: 0.9937 - val_loss: 0.5944 - val_accuracy: 0.9070\n"
     ]
    }
   ],
   "source": [
    "odin_rn_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = odin_rn_model.fit(train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "QPWAgdc65Ppc",
    "outputId": "24617f95-2bf1-4e61-a469-ae0589a4441d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABFZElEQVR4nO2deZwcVbn3v6fX2fdkMtkXQiBkT0jCmgkokKCyiF5kEdFrRJGLC964vYh6feGKryKIcPEaFRCCiihLUAQyJCwhISEJ2SA72TMzyexbL+f941TN9PR0z3RPema6p5/v59Ofqq46derpyuRXTz3nOU8prTWCIAhC6uMYaAMEQRCExCCCLgiCMEgQQRcEQRgkiKALgiAMEkTQBUEQBgmugTpxSUmJHjt2bK+ObWxsJDs7O7EGJYhktU3sio9ktQuS1zaxKz56a9f69eurtNZDIu7UWg/IZ/bs2bq3rFy5stfH9jXJapvYFR/JapfWyWub2BUfvbULeEdH0dUeQy5KqQyl1Fql1Cal1Fal1A8jtClXStUqpTZanzvjvu0IgiAIp0QsIZdW4CKtdYNSyg28rpR6UWu9Jqzdaq31xxJvoiAIghALPQq65eI3WF/d1kemlwqCICQZSscw9V8p5QTWA6cBD2qtl4btLweeBg4Ch4E7tNZbI/SzBFgCUFpaOnv58uW9MrqhoYGcnJxeHdvXJKttYld8JKtdkLy2hdqllCI7Oxun0znAVplxQqXUQJvRhZ7sCgQCNDY2Eq7RCxcuXK+1nhO101g/QAGwEpgStj0PyLHWFwM7e+pLBkX7F7ErPpLVLq2T17ZQu/bs2aMrKyt1MBgcOIMs6urqBtqEiHRnVzAY1JWVlXrPnj1d9nEqg6Jh4l8DVACXhW2v01o3WOsrALdSqiSevgVBGDy0tLRQXFyclJ5xKqCUori4mJaWlriOiyXLZYhSqsBazwQ+AuwIazNMWf9ySqm5Vr/VcVkiCMKgQsT81OjN9Ysly6UM+IMVR3cAf9JaP6+UugVAa/0wcA3wZaWUH2gGrrUeDYT+oO4wHN4IZyweaEsEQRhAevTQtdabtdYztdbTtNZTtNY/srY/bIk5Wutfaa3P0lpP11rP11q/2deGCyFseBT+dCPIPVQQAKipqeHXv/51r45dvHgxNTU1Mbe/6667+NnPftarcyUaqeUyGPC3QNAPwcBAWyIISUF3gh4IdP//ZMWKFRQUFPSBVX2PCPpgwBbyQNvA2iEIScK3v/1tdu/ezYwZM/j+979PRUUFCxcu5LrrrmPq1KkAXHnllcyePZuzzjqLRx55pP3YsWPHUlVVxb59+zjzzDP54he/yFlnncUll1xCc3Nzt+fduHEj8+fPZ9q0aVx11VWcPHkSgPvvv5/Jkyczbdo0rr32WgBef/11ZsyYwYwZM5g5cyb19fWn/LsHrDiXkEBsQQ/6BtYOQYjAD5/byrbDdQntc/LwPH7w8bOi7r/nnnvYsmULGzdupL6+nvXr17N27Vq2bNnCuHHjAFi2bBlFRUU0Nzdz9tln88lPfpLi4uJO/ezcuZMnn3yS3/zmN3z605/m6aef5oYbboh63s9+9rM88MADLFiwgDvvvJMf/vCH3Hfffdxzzz3s3bsXr9fbHs65//77efDBBznvvPNoaGggIyPjlK+LeOiDgaDfLAP+gbVDEJKYuXPntos5GEGdPn068+fP58CBA+zcubPLMePGjWPGjBkAzJ49m3379kXtv7a2lpqaGhYsWADATTfdxKpVqwCYNm0a119/PY8//jgul/Gj58+fzze+8Q3uv/9+ampq2refCuKhDwZsQRcPXUhCuvOk+5PQUrUVFRW8/PLLvPXWW2RlZVFeXh4x59vr9bavO53OHkMu0XjhhRdYtWoVzz77LD/+8Y/ZunUr3/jGN7j66qtZsWIF8+fP5+WXX+aMM87oVf824qEPBto9dBF0QQDIzc3tNiZdW1tLYWEhWVlZ7NixgzVrwmsNxk9+fj6FhYWsXr0agMcee4wFCxYQDAY5cOAACxcu5Kc//Sk1NTU0NDSwZ88epk6dytKlS5kzZw47duzo4Qw9Ix76YEBi6ILQieLiYs477zymTJnCxRdfzFVXXdVp/2WXXcbDDz/MtGnTmDRpEvPnz0/Ief/whz9wyy230NTUxPjx4/nd735HIBDghhtuoLa2Fq01X//61ykoKGDp0qW88cYbOJ1OJk+ezKJFi075/CLogwHx0AWhC0888QQA9fX15ObmUl5e3r7P6/Xy4osvRjzOjpOXlJSwZcuW9u133HFHxPZ33XVX+/qMGTMievuvv/56l20/+9nPyM3N7elnxIWEXAYDIuiCICCCPjiQQVFBEBBBHxxI2qIgCIigDw500CzFQxeEtEYEfTAgMXRBEBBBHxyIoAuCgAj64EAGRQWhE6dSPhfgvvvuo6mpKeK+8vJy3nnnnV733ZeIoA8G2qstiqALAvStoCczIuiDAfHQBaET4eVzAe69917OPvtspk2bxg9+8AMAGhsbufzyy5k+fTpTpkzhqaee4v777+fw4cMsXLiQhQsXdnueJ598kqlTpzJlyhSWLl0KmHrrn/vc55gyZQpTp07lF7/4BRC5hG6ikZmigwFJWxSSmRe/DUffS2yfw6bConui7g4vn/vSSy+xc+dO1q5di9aaT3ziE6xatYrKykqGDx/OCy+8AJgaL/n5+fz85z9n5cqVlJREf9f94cOHWbp0KevXr6ewsJBLLrmEv/3tb4waNYpDhw61zzK1y+VGKqGbaMRDHwyIhy4I3fLSSy/x0ksvMXPmTGbNmsWOHTvYuXMnU6dO5eWXX2bp0qWsXr2a/Pz8mPtct24d5eXlDBkyBJfLxfXXX8+qVasYP348e/bs4bbbbuMf//gHeXl5QOQSuolGPPTBgLyxSEhmuvGk+wutNd/5znf40pe+1GXf+vXrWbFiBd/5zne45JJLuPPOO2PuMxKFhYVs2rSJf/7znzz44IP86U9/YtmyZV1K6CaiwmM44qEPBiTkIgidCC+fe+mll7Js2TIaGhoAOHToEMePH+fw4cNkZWVxww03cMcdd7Bhw4aIx0di3rx5vPbaa1RVVREIBHjyySdZsGABVVVVBINBPvnJT/LjH/+YDRs2RC2hm2h69NCVUhnAKsBrtf+L1voHYW0U8EtgMdAEfE5rvSHh1gqRkZCLIHQivHzuL3/5S7Zv384555wDQE5ODo8//ji7du3iW9/6Fg6HA7fbzUMPPQTAkiVLWLRoEWVlZaxcuTLiOcrKyrj77rtZuHAhWmsWL17MFVdcwaZNm7j55psJBs0M7rvvvjtqCd1EE0vIpRW4SGvdoJRyA68rpV7UWoc+LywCJlqfecBD1lLoDyRtURC6EFo+F+D222/n9ttv79RmwoQJXHrppV2Ove2227jtttsi9ltRUdG+ft1113Hdddd12j99+vR2Tz+U8BK6iXgpdDg9hly0wX42cFuf8ODRFcCjVts1QIFSqiyxpgpRkRdcCIIAqGiB/U6NlHIC64HTgAe11kvD9j8P3KO1ft36/gqwVGv9Tli7JcASgNLS0tnLly/vldENDQ3k5OT06ti+ZiBsO+fNz+Ntq2bfmGvZN+4zSWNXLIhd8ZOstoXalZ+fz2mnnTbAFhkCgQBOp3OgzehCLHbt2rWL2traTtsWLly4Xms9J+IBWuuYP0ABsBKYErb9BeD8kO+vALO762v27Nm6t6xcubLXx/Y1A2LbT0/T+gd5Wv/rrqhNkvWaiV3xk6y2hdq1bds2HQwGB86YEOrq6gbahIj0ZFcwGNTbtm3rsh14R0fR1biyXLTWNUAFcFnYroPAqJDvI4HD8fQtnAIyKCokGRkZGVRXV0dN7RO6R2tNdXU1GRkZcR0XS5bLEMCnta5RSmUCHwH+O6zZs8BXlVLLMYOhtVrrI3FZIvSe9kFRSVsUkoORI0dy8OBBKisrB9oUWlpa4hbG/qAnuzIyMhg5cmRcfcaS5VIG/MGKozuAP2mtn1dK3QKgtX4YWIFJWdyFSVu8OS4rhFNDPHQhyXC73YwbN26gzQBMVsrMmTMH2owu9IVdPQq61noz0OWslpDb6xq4NaGWCbEj9dAFQUBmig4O2j10CbkIQjojgp7qaA1aJhYJgiCCnvrYA6IgxbkEIc0RQU91dIigS8hFENIaEfRUJ1TEJeQiCGmNCHqqEyrokrYoCGmNCHqq0ymGLoIuCOmMCHqq08lDlxi6IKQzIuipTqcYumS5CEI6I4Ke6sigqCAIFiLoqY6EXARBsBBBT3VkUFQQBAsR9FTH9sqdXklbFIQ0RwQ91bEF3Z0h9dAFIc0RQU917JCLK1OyXAQhzRFBT3VsQXdnSMhFENIcEfRUxw65uDIl5CIIaY4IeqrTLugyKCoI6Y4IeqrTPiiaJWmLgpDmpJygbzxQw282t3KsrmWgTUkOQrNcdACCwYG1RxCEASPlBP1ITTNvHPZzolEyOoDOWS4gYRdBSGN6FHSl1Cil1Eql1Hal1Fal1O0R2pQrpWqVUhutz519Yy543cbkNr94okBnDx0k7CIIaYwrhjZ+4Jta6w1KqVxgvVLqX1rrbWHtVmutP5Z4EzvjdTkBcB9+BzxlUHpWX58yuQnNcgHx0AUhjenRQ9daH9Fab7DW64HtwIi+NiwaXpcxeewb/wkv/3CgzEgeunjokrooCOlKLB56O0qpscBM4O0Iu89RSm0CDgN3aK23Rjh+CbAEoLS0lIqKinjtZW9tANB46g9Qi4t3e9FHX9LQ0NCr39Vbhh57j8nAh0erGA28+cYq2rzFA25XrIhd8ZOstold8dEndmmtY/oAOcB64OoI+/KAHGt9MbCzp/5mz56te8OOI3V6ztLHtP5BntYPzOlVH33JypUr+/eEG5801+KV/zLLE/uSw64YEbviJ1ltE7vio7d2Ae/oKLoaU5aLUsoNPA38UWv91wg3hTqtdYO1vgJwK6VKTv120xWvy8EoVWm+tNT2xSlSCxkUFQTBIpYsFwX8Ftiutf55lDbDrHYopeZa/VYn0lAbr9vBSFvQm2v64hSpRfugqCXoMigqCGlLLDH084AbgfeUUhutbd8FRgNorR8GrgG+rJTyA83AtdajQcLxupwdHnqgFXwtHd5pOhIu6OKhC0La0qOga61fB1QPbX4F/CpRRnWHCbkc79jQUntqgu5vhdqDUDzh1I0bCNqrLUraoiCkOyk3U9TrCgm5ALTUnFqHm56Eh84DX/Op9TNQtMfQLUGXtEVBSFtSTtBdTjMo2uLMNRtOdWC0sRL8zakv6DKxSBDSnpQTdIIBhqtqjmadbr6fqqD7rZowwRT1bLtkuUiNG0FIV1JP0OsO41YBDmdMNN9PNdPFFsBUFcLw4lwSchGEtCX1BL1mPwAfek8z31tqYOfLsPNfveuvXdBTNFQR+oILkJCLIKQxcU39TwrqjwKw32MLei28+iNwemHiR+Pvz99qlqkccnG4wOk231P1xiQIwimTeh761Gsod/6BDx0jTZihpQZqPgRfU+/6S3kPPQDKCQ5L0FP1xiQIwimTeoIOaKeXFj+QkW9yyJtPQltj7zqzBT1VQxXioQuCYJGSgu52QKs/aAT96Bazsbdph3bIpa8HE7c/D/dN68iqSRTBQJigt0JjVWLPIQhCSpCygt7mD0JmAVTvMhtPOeTSx1kulTvMgG5bQ2L7DfrBERJyWfMQ/HJ66ubVC4LQa1JU0BWt/oDx0LFKxrQ1Qm/Kx/RXyKX9SSDB5wkPuVTuMDeN5pOJPY8gCElPagq6MyTkYqMDvRPLvhLacAKtnZeJwhZ0R1jCUmt9Ys8jCELSk5qC3h5DL+i8w9eLgdFAP80U7TMPPSyGbtNSl9jzCIKQ9KSooCtafYHOHjpAWy/i6P2VtmgLur8vPHQnOD2dt7eKoAtCupGigh4WcvFay94MBPr7O4ae6CwXO+TiBBTkDjfbJeQiCGlHSgq6yxb0zAKzYeiZZtmrkEs/pS0G+ljQAc66Cs7/mlkXD10Q0o6UFHS3Q5m0RdtDH3qGWfYm5OLvp7TFvvLQddDyzoFP/Q6mX2vWxUMXhLQjNQXdCW2BIMEhkyGnFMacZ3b0Jhe939MW+8JDd3Z89+SYpQi6IKQdqSnoltVtBePhjg9giOWh90rQe5l9cmgD7KmIvb2/xVr2YcgFjLh7ciTLRRDSkBQVdPOK01Zf0GzwZJvlqYRc4k1bXPUz+Md3Y2/fVzNSwwUdwJsnMXRBSENSVNDNstUf9oLkUwm5xOuh+1vMq+viaR96vkRh56GH4s0VQReENCQ1Bd0KGbf6LQ/dnWWW8Qp6MNgRO483hh5oiy+nvL9i6GAJusTQBSHd6FHQlVKjlFIrlVLblVJblVK3R2ijlFL3K6V2KaU2K6Vm9Y25BpcdcvGfYsglVFwDPpPHvntl7Mf2RtD7ZGJRmIeekSeCLghpSCweuh/4ptb6TGA+cKtSanJYm0XAROuzBHgooVaG0SXk4nQbUYvXQw8X9K3PwGNXtr8Vqcdje+Wh91FxrlC8uTIoKghpSI+CrrU+orXeYK3XA9uBEWHNrgAe1YY1QIFSqizh1lp0CHowZGP2qQl60NchgrGIob8tvkJb/TGxyEZCLoKQlsT1TlGl1FhgJvB22K4RwIGQ7wetbUfCjl+C8eApLS2loqIiPmst/K0tgOLtdRuo22Pix+doJyf27+L9OPr0tlRxjrV+YP9e2jw1TADeWbOahtzD3R47t76GrEAbFStfBdVxX2xoaIj4u85vbcQF7Nm5gw/bYrexJ+bU19HsP8nWkHNOqKyjrOkkr4dsi2bXQCN2xU+y2iZ2xUdf2BWzoCulcoCnga9prcNdWBXhkC7FybXWjwCPAMyZM0eXl5fHbmkIu/72CtDCmVOmUj5pqNm4uYCyknzK4unzxB5YY1ZHDR8G2UNhD8yZNhnGnNP9sRtd0Azl55/TkWUDVFRUEPF3rTLhofFjRjK+l787Ilu85Awt7XxO/RYcfJbyCy9oHzCNatcAI3bFT7LaJnbFR1/YFVOWi1LKjRHzP2qt/xqhyUFgVMj3kUD3Lu4pED3kEmdxrtB4dsDXERaJJXRjh05iiaOHZtMkfFA0StoiJP7tSIIgJDWxZLko4LfAdq31z6M0exb4rJXtMh+o1VofidL2lLEnFrWFCronK/4XRYeKa9DXkSsey40hHkEPjbX3Rww9I88sZWBUENKKWEIu5wE3Au8ppTZa274LjAbQWj8MrAAWA7uAJuDmhFsaQmQPPTN+QQ/PcrFnjcbkodsed0vPbUPb9NfEIpCBUUFIM3oUdK3160SOkYe20cCtiTKqJ7qkLYIJucT7tvtwQe8p5LLtWVPd8Kwr45so5O9rDz3CxCIQQReENCOuLJdkwe0Mq+UCCQq52IIeJeSy5iEjoJOvCAm5xOKhhwp6f+ShW2WFZfq/IKQVqSno0UIupzSxyB8i6FH68beYl1EHA7Qn8cQSQw9t0x8zRds9dBF0QUgnUlLQXdFCLvFmudji6vQYDz1ghS6i9eNvNYIeiFOg+3RQtJsYugyKCkJakZKC7lAKj9MROctFa1DdhPwDfji5z8SdbXH1ZHcOhUQVdMtDDxXluEMu/RBDt7NcJIYuCGlFSgo6gMfl6BpyscXW5Y18UMAHv5wOdYfAkwuL7rGOtQRdW/1Fi8W3e+i+ztt6oi8Eva0JWmoih1zc2YASQReENCMly+cCeF2OriEX6D6O3lhlxDx3OLTVQ1O12e7JtkIudtpiNx66vyVMoGMRdMuLd7gS98aiVT+FB+cZu8M9dIdDaqILQhqS2oIenuUC3ZfQtQV82BSzbDhudZZj5aHbE4uiDYq2WkW52jpv64n20E5O4jz06t0dgh3uoQO4MmILBwmCMGhIXUF3O8NCLmEvuTix18TTQ2k+YZZF482ysbLj2KA/ZGJRDx563DF0q403L3GC3nCsYz3cQwcTdkp0iqQgCElN6gp6l5BLiKBXfgAPzIIP/tH5INtDL5pglrYoenLCJhZFEPRgwIQ3dKCzBx9LCMX24r25iRP0+qMwbJqp9OjN67rf6U78AKwgCElNyg6KZnmcNLaGCHpoyOXAWjPAWb2r80Htgm556A2VHcd2mlgUIeQSGloJHWyMJ8vFmwtNIbNZj2yGzEIoGBX5uGhobW5GZ34crvofKBzbtY3Tk/icd0EQkpqU9dALsjzUNoeEFOxB0eaTsP8Ns14XVh+syQ65jDPLxuPgcBvx62liUahwh+Z3+1uh/hhUvh/dWLvfjLzOHv1fboaV/zf6cdFoqTX25A6D0skdN7NQnG4JuQhCmpGygp6f6aamOUQch001oZMdL8D+N822+rAKvk3VJjyRPcR8b6wyYm6HJ9qzXOLw0AOtsPK/4Il/i26sHcoJHxRtOmFuQPFih4pyhkVv4/RIyEUQ0oyUDbnkZ7qpaQrxQD1ZMPlK2PxUR+3x8HeDNp2ArCIT+lAOE5ZxeYyX3lP53FAPPTzk0lgF9d1UC24fFM3tnObY1tC7muX278rtTtC9IuiCkGakrIdekOWmvsVPIBiSyTLjMx1iPmwa1EXw0LOKzUzSDKuAldPbEZ7oLg89agy91Xz3t0RPmfSHpi36OrYF2uIvKAYdHnq3gi4hF0FIN1JX0DPdANSFxtFHnwsFY4xoj19gPNnQ1EVb0AEyCszS6TF53LawKqcJuYSnPIZ61q1hMXT7u50WGY6/xeSFu0LCILZn3htBtz30nNLobZye+F5iLQhCypOyIZeCLA8ANc0+CrPNOg4HfPw+M2hYf9QIWvNJE2YBE3IZeqZZzyyAkxiRdbpNOqK9vam6awmBnjx0u/9IBNqsJwErDKL1qQl6wzGTpmkX4YqE0yMeuiCkGSnroedbHnpNU1iceMJFcNZVHeGI0LBLUzVkWuLe7qF7TQzdJrPQLMOFtlMM3fLI3dlme7ugV0c21t9ibg5O6zwBH7Ragu6LIOgH1sGHb0fuC8zNKqe0+yJkLhkUFYR0I3UFPcsS9OYoXmjucLO0Byt9LUY8bW/djqHbHrqNLejhcfRIHro9UcgW56ghF8vbtz3+QGv3Hvq/7oR/LI3cFxgPvbv4OUiWiyCkISkr6HYMvbYpiqDnlZmlLei22Nox9MwCs3SGCbrtuXcR9Ah56Bl51oCo1TZayKXdQ7dCQwFfx00h0NZ1tmljJdQejNwXdHjo3SGDooKQdqSuoFsx9NpoHrqdo21PLrLDIREHRSN56GEZK9E89NAwS1RBb+3IprG/h3rm4WGX5hNG1KPN9IzVQ5eZooKQVqSsoOdlmPHcmmgeussDWSUdk4vCBd320ENj26Hbu/PQQwU99MXU0UIugVbrPHbIpa1z/nmouAcDHZON6g517autycTwe/TQZVBUENKNHgVdKbVMKXVcKbUlyv5ypVStUmqj9bkz8WZ2xeV0kOt1dZ4tGk5uWUeKXxcP3c5D93QuP9secgn30MMGRZXDDIqG1mbpzkPvEnKJIugttR0v2qiNIOh2hcicoZHPZSMxdEFIO2Lx0H8PXNZDm9Va6xnW50enblZs5Ge5o8fQwcTR7SwXW2yzwrNcog2K9hBycXrMU4AOKeEbNcvFEnSXLeit5gUbNqHeeuhNIZKHbt9Askoin8tGBF0Q0o4eBV1rvQqI4noOLAVZ7uhZLhDmoVs/wRbs0JBLxBh6NyEXHTDhE1dG5+N6mljU7qGHzRANXQ+9KUQaGG0KG9yNhtNj7AwGum8nCMKgIVETi85RSm0CDgN3aK23RmqklFoCLAEoLS2loqKiVydraGigoqKCYEszHzY2RO1nbHUrYxorWfXqy5y2ayNDXdm8sdpUYsyt281s4Mjxak76djLZOua9XQeYCuzY8i5Hqzu84LF732csoFEoNG1BqDp+Ais5kjpXMe4Th9ttC+XsupM0+bM5vHUH04EN69Yw9PgORtrnXL+G6v3G0y+uWstUa/uhHevYGezcV+nR1zkTePu9XTTvjj4pafT+A4wHVq18maDTG9GuZEDsip9ktU3sio++sCsRgr4BGKO1blBKLQb+BkyM1FBr/QjwCMCcOXN0eXl5r05YUVFBeXk5fz60gR1H64jaT+4+2L+cBbMmwTEF/jEdbatHwQYoGzmGsvHTYbvZPHXuhbDlJ5wxfjRnzDkfnNYl8r0KhzJQKPA348nIZviosWAl0eSNmgK7XyUnJ6fjHNuehXW/AZcmu2wUQ2bOgc0wa9oU2PQeWBGVqZPGw1TrmHcPwhYgI58R2ZoR4b/tra2wA+YtXNTxlBGJN7fAXrjwvPmQkd9+zZINsSt+ktU2sSs++sKuU85y0VrXaa0brPUVgFsp1UOANzHkZ7mjpy1CyOSio1B7APJGdOyzQyvhIRc7tr79Obh7ZEfaY3sc3MpUcXk6lwYoHAOtdahgiD3bn4O9q6DuYOf2AatcgP2moUgx9GHTTAzd3wb73oBNy00YqKna1JuxB3Wj0X4uyXQRhHThlD10pdQw4JjWWiul5mJuElFGBxNLgVVCV2uNijQN3s7Vrj9sxHHE7I597Vku7rCJRZbI7lttlif2mMFVOw5u4/SEfFeQb9465PaFiPPxbR3rrozOU//bGk3qYWtd1xi6ww1DJsF7f4E/fw7ef6HzfrtiZHe0n0sGRgUhXYglbfFJ4C1gklLqoFLqC0qpW5RSt1hNrgG2WDH0+4FrtQ4vVdg3FGS58Qc1jW1RBv7yLA/9xF4jhPkhHrrDCYvuhemf6Zy26MroeD8pdGSVhHvozpCsFW9e+yCly2/PAPWZtxiF1ozpNCja0JFLHirozVbN9rwR0FJjxHzel82+mg87V4zsjtBzCYKQFvTooWutP9PD/l8Bv0qYRXFgF+iqbfaR443wU7KKjbd76B3zPW9k5/3zlphlaGaJy2sE3U5btCcO+VuMKNuesdPd4aF7c9vTId0+qyxA1U5Tm33BUnjlh2a/PbHIb9V/KRpntoWHXLKKId+yNbMILvo+vPdnk/Vi7++J0Jx3QRDSgpQtnwuQn2lE62RjGyMKMrs2UMqkLh5YZx0womsb6BxDd3rN249yJpuQiS32/lYj4HakIzTk4s1tF1m3z/LQj1mJPuMXwKQ1ZiKQfXMItJk8dE8OeLK7hlwyi6BgtPl+zq3gzTG21x0y+0tO7/nihJYZEAQhLUjZqf8AQ/OMx3u0tiV6o7wyaLBy0fNHRm5jZ7Iop1m/7B644lfgze/sobtCcs9dng4vOETQPW01ZtvxrSaUUzzReOKe7LBqi41GqD05nd90ZL8mb+RcuPp/4dzbLNtHmZmjMYdcQsoMCIKQFqS0hz6uOBuAfdXdvCQit6xjPS+Kh24Lsy24Z1xultnFHVPt2z101XFMqIeeMwxcmWQ2WzNTj22DkkkdcfbQ89hT/z3Z5mmgU8jFEmyHA6Z9qrPte14zoSB7tmt3hA7ACoKQFqS0h16Y7aEgy83eqhgEPXtI5zTDUOyQi9PTeXtWSYyDorlGgEtOI6vJmt15bCuUTu7cn91/W4Px0j25nUMuwWDnNyyFkj/ChGl0QAZFBUGISEoLOsDY4uzuBd2uix7NO4cObzY0LREguwQaw2LodpvwQVGAkklG0JtOmNzz0rPCzmOJrJ1r7g2LobfWRhfs0HCRCLogCBFIeUEfX5LNvlg89Gjxc+hIW3SFe+jFIR56S5iHHjJRyJ4gNGQSGS2VsPc1833k2Z37s28cdnlcjx1Dt+xvrzcTwUPPi1fQJQ9dENKNlBf0sSXZHK5toTlaLnosgm6LnzMsJJNdYmLaWnf10F2ejvbtHvrpKDS8+0czwDp8Vuf+lDI3gi4euhVD767wVmiGTiwxdJcMigpCupHygj6uxAyM7j8RxUu3Jxd1F3JxRAm5ZJVA0G8m+IS/Ri58UBTM7E6A3a9A2TQz4BmO09NRlTE8bdEegM2OIOg5w8xNAiQPXRCEiAwaQd9bGUXQC8dB+Xdhyiejd+KMEnLJtkrSNFZHiKGHhlwsQS+agMZhaqSPmh/5XJ4cqN7VsR4acrHrn0e6+ThdHU8bEnIRBCECKS/oY21Bj5a66HBA+dLok4ogxOuOEHIBE0ePFEPPLTPx8yFnmG0uD82ZVv2YUXMjn2velzomK9khF1+jyXCpO2Ti+dlR3kaUP8Kc15MT/beE/yYRdEFIG1Je0HO8LobkeqN76LHQHnIJE3T7rUCNldZ7QcM89Oxi+M4BGD2v/ZCmLCtWP2oeETn3Nii1Kp7bIRcw+eW1h0yFSEeUf5aCMab+S0+FuWz7wJQZEAQhLUjpiUU2E4Zk8/6x+p4bRsMZRdBtD91+t2fo/ig57VUlcykpzIv+ROB0w9WPwNpHzOxPW9DbGs3r8rp7krjoe9BwvIcfY59HPHRBSDdS3kMHmD++mPcO1VLd0Mu6JUpZ0/4jTCwCk1MOXfPQI3C07KNww9Pdn690Mnz8PhMXt8MnbQ3mPN0N3haOjR7KCUcEXRDSjkEh6AsnDUVrWL2zqvedhE4UsnFnGMEN9dBDY+iJoP0p4KDx0O2snFNFpv4LQtoxKAR96oh8irM9VLwfYzgiEg531ywXMBkldvZJeAw9EQybZpa7XzXedHf58vHgcJqnDvHQBSFtGBSC7nAoFpw+hFU7qwgEe/lujYz8jpdRhFJ8GhzZbNb7wkPPGWoGQt9fYb4nykMHY2NAyucKQrowKAQdYMGkIZxobGP9/pO96+DGZ+CCb3bdPvUa8Deb9b7w0AGGz4CqD8x6dzH0eHF5JOQiCGnEoBH0i88spSjbw30vf0Cv3oA35PTIU+rP/HjHK+nCp/4nirLpHeuJCrmA5aFLyEUQ0oVBI+g5Xhf/cdFpvLm7mtc+qExcx95cI+pghVxCpv4nClvQHe6OzJpEIIIuCGnFoBF0gOvmjWFMcRZ3PbuVE40JFLLZNxtxzB/RkTfujvDKu95SNsMs87qZVNQbnG4JuQhCGjGoBN3jcvD/PjWdI7Ut3Pz7dTS2+hPT8Zhz4LuHoWg8lM2EKx+GcQsS0zdA7jAz3T+R4RYwNyF5p6ggpA09CrpSaplS6rhSakuU/Uopdb9SapdSarNSalakdv3FnLFF/Oq6WWw5VMsNv32b2qYEeah2XrfDATM+E3ViUa9QyswCnfelxPUJpjaNeOiCkDbE4qH/Hrism/2LgInWZwnw0KmbdWp8dHIpD143i62H6rj8gdU8+tY+WnxR6qUnC7M/B5OvSGyfTrfE0AUhjehR0LXWq4AT3TS5AnhUG9YABUqpsm7a9wuXTRnGY1+YS0mOlzv/vpWLflbB42v2c7imeaBN6z9kUFQQ0opEFOcaARwI+X7Q2nYkAX2fEvPGF/O3W8/jrd3V3P3idr7/NxM1Kp80hJvOGUtjm58pw/PbS/AOOmRQVBDSChVLzrZSaizwvNZ6SoR9LwB3a61ft76/Avyn1np9hLZLMGEZSktLZy9fvrxXRjc0NJCTE0NN8BC01hxs0Lx73M8/9/loDNG58fkOcj2K6UOcLBjp4mijJsejyPfGUKY2Abb1FdM23YXL38iG2fcmlV2hiF3xk6y2iV3x0Vu7Fi5cuF5rPSfSvkR46AeBUSHfRwKHIzXUWj8CPAIwZ84cXV5e3qsTVlRU0NtjbwRqm3xsPVJLXoabV3cc5/WdVVQ3tvLotkb+ujtIQ6sft1OxaEoZE4fmMCTXy5BcL0NzMxiS66Uo20MgqHE6FB5X56jVqdiWcI6UQe0BysvLk8uuEMSu+ElW28Su+OgLuxIh6M8CX1VKLQfmAbVa6wEPt3RHfpabcyeYCTxTRuTzHxdPRGvNiveO8q9tR5k7rpj3j9bx3OYjPLsp4r2pnRyvi4IsN6V5GUwYkk1ui4/SI3U0+wI0tQZwOhSTy/LYcbSOgyebWTR1GJluJ76A7nIzSDgSchGEtKJHQVdKPQmUAyVKqYPADwA3gNb6YWAFsBjYBTQBN/eVsX2JUorLp5Vx+bSO8dwfXjGFVn+AqoY2Kutb2z8nGltxOhwEgkFONvk42djGoZpmXt5+nBONbfx2y+qo5/nJiu14XQ6O1bUwa3QhuRkuapp9zB9fTFGWhw9PNOEPBq0nAAcXTCzBoRT/2HKEbK+LCUNyOHtsEfmZbjLcDopzvDS0+jnR2IZDgUKhFDgdijKnByWDooKQNvQo6Frrz/SwXwO3JsyiJMPrcjKiIJMRBT3PDNVa8+hzr1IydjJZXic5XhdNbQG2HKplTHEWxdleHluzD6UUIwsyeWtPNcfqWsnyOHlk1R4CQU1ehosMtxOnQ9HY6ufJtR8CUJztIaA1NWF59Q4F0QpMPj+qmSnioQtC2jAoXkGXLCilGJPnpHxa56zNBacPaV8/Z0JxxGPrWnz4A5qi7I4aMYGgZs2eanyBIOefVoLL6eBwTTMbPjxJiy9Isy/AsdoW8jJdlOSYsr5BbW4sv67YzfGmICjx0AUhXRBBTxLyMrrOPHU6FOed1rlY1/CCTIbH8LTw1u5qqt4HPDL1XxDShUFVy0XoYMLQHGraFFpCLpHZ9Qo097J2viAkKSLog5TThubgw4mWQdGuNNfA45+EN+4faEsEIaGIoA9SjKC7cAR90JsXfiQzm5bDvjd6f/zJfYCGA2sTZZEgJAUi6IOUMUVZBJQVlx9MYZdgEF64A1777973UWMyhzi8AQIJKrEsCEmACPogxeV0kJNlvTpvML0ounoXtNXD4Y29f/Ko2W+WviY4FrEqtCCkJCLog5iCXKvo2EB76Gsehj98Alb8p4lfnwqH3zXL1lo4sad3fdR8CMpp1g+uOzV7BCGJEEEfxBTmmcI/bUe2Mnr/n+C+qXBsW+TGAV904Q/44fmvw6ENvTNkw6NGiNf+D2z+U8/tm2ugalfnba/8GD74pwmT2BzZaGyK9puicXI/DJ0MOcPgwNvxHSsISYwI+iDGPeFCTugcPI99jPF7/wh1R+CZL0UW7r99GR67KnJHm5fDO8tg61/jN0JrMwg543ooPg0+eLHr/u3Pd3ju7/0FHpgND58HLXVm24m9sPpn8M/vwqH1MGKOeRvTvteNzcsug6qd8OYDsOahrqGYgL/zq/hqPoTCMTDqbCPog23QWEhbZGLRIGby1NmUP/dzHp20hoyMbM6YMQ+eugFevw8WfKujYWMVbH3GhCECfnCG/FkEfB0DkNW74zeisQp8jVA4Fk6/DNY+Aq314M0FYPSHT8Nrj8Hpi+DsL8DTX4CCMdBUBR+ugdMvgS1PW+e3vPb5XzHL9X8AHQBPDjx0XsdYweF3ofaQeVfrNb+FF75hQitfftPsr9kPExbCkEmw/TnY9ncYXw4n98LwmfH/RkFIEsRDH8SU5mVQUDSUXzuv52jZR+DMj1ui+j+dvfT3/gxBvxHEk3s7d7LxCePR5pT2TtDtAcjCMTBpkXmD0u5XTZ+v/4Jxex83L9/+4EV46kYTCvnSa+ZtS3tfM8du+SsMn2VepA1GdIfPNGI+ah5c+0cj3h+/H+bfCpufMuGYLX8xYZqNT8Dxbea3NVWbwdCCMTDjBhg2FV5cCo+Uw28uhqbuXs4lCMmNCPog5+yxRbyz/yTtLzKZdRM0VpqZkjYbn4CMArN+PCweveUvUDIJpv2bEcRgAFpqoeF4bCl/J/eZZeFYGDXfnOe5r5l4/st3UVNwFnxpNYw+xwj01Y9AZqER6r2r4Ph2OL4Vpn8G5nze9DVitvkAnPNV411/bTPMvgku/Ql8+S342nvgzYO/fB6C1s1rT4WJnwMUjDZPIh/7JTQcg9oD5vy9uWkJQpIggj7ImTuukBONbRxptAR94kchqwQ2PWG+H1gLRzfD+V8DlBFQm5Y62P8WTLrMxL8DbSaeff9M+NlE+PkZRty7wxb0gjFGQGfeABl5sPD78NX1bJrxE/DmwPV/hlveMB4zwLgL4eh7xnt2uM0LtC/4Btz8IhRPgCmfhM88ZZ46QlEKSidDVhHM+iy0NcCEiyFvpBH00CcGgJGz4fP/gBussE74E4ogpBAi6IOcs8cWAfDByYDZ4HTDtE/D+y8ab/SFb5psjzlfgKJxnT30va8Z73biJUbQwYQzmqphyjXG09/6NzOoGC1UcXKfCdd4rJz4S38Ct28yMfyS0zraeXNhyOkd38ddCGhjw6U/gdxScHlhzLlmv8tjbjSqm9cEzv+KsfuCbxovfs9rHamO+SEv2Ro93zwRgLlhCUKKIoOig5xxJdkMyfWy9qgPrTVKKRO62PAYPDjPCPY1vzNe89DJxkPf8xoc2QSVO0zYYtQ8I+JgUhBRsPhe49lvehIaj8OrP4GL74Tzv95ZZGv2G+88XkbMNk8Sp30E5i7p3Y/PHwG3Wa+2rTsMGx+HiruNyGfkdW7rzoTc4eKhCymNCPogRynFlxdM4EfPb+PZTYe5YsYIKJkIt6yGv38VcobAWVa64tAzjef+55s6KhGe+Qnj1eeUmmyShqNQNsOENKZ/Bl75ockgyRlq1puqjUd9ZDN4so2HPmp+/IY73XD7RnPO7rzwWBlfbm5Oo+ebwdNIFI0XD11IaSTkkgbcdO5YxuU7+NFz26hpsqovFo2Dm1+AT/2+QzCHnmkGBtsa4aLvgzffDIaCaVM03qyPu9Asp/0boIwn/ZU1ZsB1za9h01Ow7FL43SKTPlg4tneGe3MTI+Zgblzf2mVi9XllkdsUjR18Hvq2Z+HDCJOnWhvA19LxXWszT0FrM1nr1+eawWt70lb9sa6TvYSkQzz0NMDpUNx8locfrmnlJy9s595PTY/csGyGWV5wB1z4LbMMFdTiCSbMMn6B+Z4/Aj7xgLkRZBXBR38E76+AZ5ZAZpHx1nWg94KeaFze7vcXjjMZL60N/WNPXxHwgcMFu1+BP91otk27FqZcbW7GLbUmRTMjH77wkhk3+dcP4MM3oXSKyQRyZ5hw2vrfQdl0I+xBH5x7G1z0f3q+lslMMGDGf+qPmn/v+qPQUgM6aPa7MswchdYG027cAnOtGiut5IBWOLbVODLNJ83ktBN7zOS1nKHmaTYjz/TXdMIMzDtc5qMD0FgN4y4AshP+00TQ04TReU6WXDiehyp2c9XMEZwb9iYkwAj2retMSAa6esfDpsHOl02Koc2sGzvWMwvg0rvhuf+Aa5bBwXdg5X919JfsFI0zSzszJ9H420woKdpTR0strP0N5A2HGdeZbUc2m7IL53/dDCw/82XOzDodhtaa/trqTSropMVQ9T68/T9mtm3ReHNDHXIGnH6p2b55ucnlzyw0++oPmyep49uMCF3wTePR54+AG/5qxhU2PGomnc3+nBH0Nx+ALc/AWVea8RGtTX+ZhYw+chLW7DC/w+U1ola928wpKD3LPHH5mqwbvQbl6Pi0NZinucwC4xy0NZm2bQ3mibGt0QhsS40R0aDfZES5M40g1x81glk4xhJObQm0ZvrhPfCulY0VaDXCbIt3vLgyzLmDYSm7GQXGlsbKrvvC8eSaa8a83tnQnXkJ71FIWm6/eCIvvneE25/ayNO3nMvo4qyujUIzTcI551YTN/d041lM+5RJMXR5jGcz7gIYefapG98fFNqCvhfIjd5uz2tmtusZl8ceEjq+A36/2Az2Xv0bI0oNx4xgtdTBvtVmRqydBnpoven/mS+bcYunbjBClT+C4up18KdVnfv35Ji+3Fkw9VNw6B1j441/NcJX/h1TQ/7NX5rlvz1mvMqXvm/af+wXRnAv+j9GDB1WNPb8r1kprRaTr4SV/9eE1oomGJuaT0LzScYHWiE8YhVNACPhygR/c9dtnmxzM/PkGsG3Q38H1pp+c4eZeQX+FitEpAFl3SwUjqCGMfMtL9lp2ueUWsthJoMqs6i9Pa31JjnAm2OEever5jxZxSZZwOWFkXPN7/ZkGQcnx5r0Fgya7a11pr/MQnNdgwFrPoQyTz8AFRU9X5M4EUFPIzLcTn7z2Tlc8/BbfHbZ2/zpS+cwNC8j9g5c3ujx507trBddOxxmEDJVsD30E3uBaZHb7KmAx68x/zknXQ7nfMUM+jotr3DHC1Bxj/Euc4aap5qC0fD2w2b/7lfh3tM6JjvZuLPMLN5zbzMe9poHYd3/moHcL75qPPemarj6N7z1xptcMHm4EXFPNlRuh/eeNqGvmTcY0QsGjahkFlj9Z8LEj5hPa4MRKzACnT+y48akVPc3qQkLzSe8RASw6pV/cuH8OcYz9zWbm1PecBMCOrHbxOxdXsguMWUmdLDj48404udvMcd5ss01cThj//eLwrsVFZSXl8d+gDuzQ6DBPLnaTPt098c6HJBdbD6hOF1drldfENMZlFKXAb8EnMD/aq3vCdtfDvydjvvzX7XWP0qcmUKimFiay7LPnc2Nv32bax9ZwxNfnM+w/DhEfTCTWQh5I+CtX1E2/BrY5zYhi6DPFDU7vt2ITclEmHoNvHYvvP+COW7sBabWzPFt5pjhM0wI4d3HTS0bbz587jkTOnjvLyYEUTjWeG+ebLNuP/mMmGXq2lTtNLHconFw1ex2MwOuLNO/TV4ZTLio829xODrEPBxbzAEKRkVu0xMRxCno9HYImdPdkRrqcJrfGwvuTPMRekWPgq6UcgIPAh8FDgLrlFLPaq3Da5au1lp/rA9sFBLM7DGFPPr5uXzud+u4+tdv8PCNs5k2smCgzUoOrn0CnrudSR88CB88aDxJT44R9clXGu914fdMnHnuEuNxv/+iqfxYPMHk+M/+nBE0MF55S635bgu2PTmqO4ondPYMBSEGYvHQ5wK7tNZ7AJRSy4ErgDiLUAvJxJyxRSxfMp8vPbaeax5+i198egaXT4shnDLYGT4Dvvgq659fxuzJ42H/G1D5volBD5vSua0314wXTL4ien9KRfeUBSHBKN1DLWil1DXAZVrrf7e+3wjM01p/NaRNOfA0xoM/DNyhtd4aoa8lwBKA0tLS2cuXL++V0Q0NDeTk5PTccABIVtui2VXfprl/Qwu7aoJ8bIKb8pEuijP7b3pCql2vZCBZbRO74qO3di1cuHC91npOxJ1a624/wKcwcXP7+43AA2Ft8oAca30xsLOnfmfPnq17y8qVK3t9bF+TrLZ1Z1dzm19/5fH1eszS5/WYpc/rL/5hnV6//8SA2zWQJKtdWievbWJXfPTWLuAdHUVXYwm5HARCR05GYrzw0JtCXcj6CqXUr5VSJVrrqtjuOcJAkuF28uD1s/hWVSNPbzjIo2/t56Vtx5g7roiZowsYX5LN5dOGk+OVpChBSGZi+R+6DpiolBoHHAKuBa4LbaCUGgYc01prpdRcTEmB6kQbK/QtY0uy+eYlk7hlwQSWrzvA42v287s39tHmD/Jfz2/nnAnFnDU8n+EFGQzLz6AsP4Nh+Zki9IKQJPT4P1Fr7VdKfRX4JyZtcZnWeqtS6hZr/8PANcCXlVJ+oBm41no0EFKQbK+LL5w/ji+cPw6tNe8eqOHJtz9k3b4TvLTtWJf2OV4XIwszmToin/xMN75AkBGFmWR6XPj8QcaWZOFQisM1LYwszKQsPwMN5Ga4aAuYP5PmtgD+YJDcDDctvgAnm9ooyfGiNQS1JsPdOR9Za40/qHE7pRyRINjE5FpprVcAK8K2PRyy/ivgV4k1TUgGlFLMGl3IrNGFALT4Ahyra+FobQtH61o4UmvW91U38sqO47T4ArgcirqWGGYGWnhWvkib30zFHpLr5WRjG/5ghz/gciguPH0I559Wgtft4M/vHGTH0TpafEFKcryU5nnJy3CTn+mmprmNXccbASjIcjOyMJOmtgBel4NZowtpCwTZeayezQdrKchyc9bwfM4/rYQMt5MMt4OLzyxN4NUThP5FnpWFuMhwOxlTnM2Y4u4LC9U2+2jzB3E6FHurGggEYXhBBh+eaKK6oQ2loK7Zz4YtOyguG0VephulYE9lI0NzvQwvyKSqoRWXQ1HT5OPFLUd5dcdxACYOzeH6eWPIzXBxuKaZqoY26pp97K5sINvr4uIzhuJ0KqobWjl4splsr4vK+lZ++cpO3E7F6KIszj+thLoWH6t3VvLMu4fa7f7rV85tv3kJQqohgi70CfmZ7vb1ouyi9vWRhZ3rxwxv3kN5+Zk99ve9y8+ksr6VE01tTCrNNS/qiJOmNj9elxOno+PYYFDz/rF6fIEgNy1bywOv7OR3N8+Nu29BSAZE0IWUQCnF0LyM+GrPhJHl6frn7nAoziwzU9T//YLx3PvP93nvYA/vSRWEJEVGlATB4rPnjCE/081dz20lEJQxfSH1EEEXBIvcDDc/uuIs1u8/yQt7fT0fIAhJhgi6IIRwxYwRXDFjOM/s9HH78nfZdTzF314kpBUSQxeEMO6+eiq+2kr+te0Yz246zOIpZcwbX8TUEflMGZEvue9C0iKCLghhZHlcfHqShx9ffw6PrN7DU+sO8MJ7RwDIdDuZNaaAaSMLGFeSzfiSbMaVZFOU7elV5o0gJBIRdEGIQnGOl+8sOpNvX3YGR+taePfDGtbuPcHavSf439V78AU6Bk7zMlxMLM3l9NJcTi/NoSw/kyG5XobkeHE5FVkeJwVZngH8NUI6IIIuCD2glKIsP5OyqZksnmpqxvsDQQ6ebGZvVSN7qhrZXdnArmMNvLjlCE+ujTygenppDqMKsyjM9jB9VAEjCzPJ9boYPySHbK8TX0CT7XGKpy/0GhF0QegFLqeDsSXZjC3JZmHIdq01VQ1tHKtrobK+lcqGVoJBTXVjG2v3nuBoXQubDtbwl/UHI/ab6XZSlp9BaZ4pfpbtdRHQmvxMN0VZHoqyPVSdCDCmqhGPy0FprheXxPQFCxF0QUggSikTasn1dtl3q6X8WmsOnmymqqGVmiZTsqDVKpNQWd/aXifn7b0naGrz43Qoapt9nUI8d6+tAMDjdFCc46G+xc+w/AzGlWTjdiocSuFyKBwORY7XRUGmm4IsDwVZbgqzPORby4JMN3mZ7k6zZ4XURQRdEPoZpRSjirIYVWTKICw8Y2gPR5ibQEOrn6qGNl6oeIsRE86g1Rdkb1Uj1Y1t5HhdHDzZzIETTfiDmmBQE9Aaf0DT2OanttlHtPqnSkFehpuCLDdelwO304HH5aA420txtgev24HH6cDrdpDpdpLhdpLpcZLpdjKyMItJw3I7lXoQBg4RdEFIAZRS5Ga4yc1wM6XERfnMkXEdHwxq6lp8nGzyUdPURk2Tj5rmNk42+qhpNtvsgmq+QJAWX5ADJ5rYfLCGtkCQNn+QFl+AaBNoh+dngL8Vz7qVZLidZHmcZHtdZFrrXpeThjY/CijO9lCU7SUnw4VDgcKUYFDW71QKHEpZ+8z33AwXOV43QW1uVN1V53Y7HXhdTlxORWOrn61VAYYersPtNH1h9WmfL/Q8mR5zw2rxBaySzpq8DBcZbme7XebfwxzjUJbN9rYBHv8QQReENMDhUFbIxQN0XykzGlprfAFNiz9AS1uAxrYA+6ob2XGknveP1nHwyDHKhhXQ3Bag2eenodVPZX0rTW0BWv0Bsr0utIbqhta4yisngnvfWd0v53E7FU6Hwu1w4HIqXE4HLofC5TTbwJSgvn7+GM7qA+0XQRcEISaUUnhcCo/LQV6GCbGMK8lm4SQTMqqoqKC8fGZMfbX5gzS3BdBogtrcLIIaNLr9pSahy/oWP/UtPpzWuIDD8orD0YAvEKTVF8QXDJLtcfHuu+8y5vTJ+IOmL439LuWO89nnarY8czuk5HQo6pp9tPqD1nGmHSF92HYHgxpfUBMIanyBIP6AeQmLPxA0y6B5ssh0Oxlfkt0n73QTQRcEod/xuEycvj9o2u+kfEpZv5wrHioq3k94n5LvJAiCMEgQQRcEQRgkiKALgiAMEkTQBUEQBgkxCbpS6jKl1PtKqV1KqW9H2K+UUvdb+zcrpWYl3lRBEAShO3oUdKWUE3gQWARMBj6jlJoc1mwRMNH6LAEeSrCdgiAIQg/E4qHPBXZprfdorduA5cAVYW2uAB7VhjVAgVIq+fKEBEEQBjGquym0AEqpa4DLtNb/bn2/EZintf5qSJvngXu01q9b318Blmqt3wnrawnGg6e0tHT28uXLe2V0Q0MDOTk5vTq2r0lW28Su+EhWuyB5bRO74qO3di1cuHC91npOpH2xTCyKNiEr3jZorR8BHgFQSlUuXLhwfwznj0QJUNXLY/uaZLVN7IqPZLULktc2sSs+emvXmGg7YhH0g8CokO8jgcO9aNMJrfWQGM4dEaXUO9HuUANNstomdsVHstoFyWub2BUffWFXLDH0dcBEpdQ4pZQHuBZ4NqzNs8BnrWyX+UCt1vpIIg0VBEEQuqdHD11r7VdKfRX4J+AElmmttyqlbrH2PwysABYDu4Am4Oa+M1kQBEGIREzFubTWKzCiHbrt4ZB1DdyaWNO65ZF+PFe8JKttYld8JKtdkLy2iV3xkXC7esxyEQRBEFIDmfovCIIwSBBBFwRBGCSknKD3VFemH+0YpZRaqZTarpTaqpS63dp+l1LqkFJqo/VZPAC27VNKvWed/x1rW5FS6l9KqZ3WsnAA7JoUcl02KqXqlFJfG4hrppRappQ6rpTaErIt6jVSSn3H+pt7Xyl1aT/bda9SaodVJ+kZpVSBtX2sUqo55Lo9HLXjvrEr6r9bf12vbmx7KsSufUqpjdb2frlm3ehD3/6NaeuFq6nwwWTZ7AbGAx5gEzB5gGwpA2ZZ67nAB5haN3cBdwzwddoHlIRt+ynwbWv928B/J8G/5VHMJIl+v2bAhcAsYEtP18j6d90EeIFx1t+gsx/tugRwWev/HWLX2NB2A3C9Iv679ef1imZb2P7/B9zZn9esG33o07+xVPPQY6kr0y9orY9orTdY6/XAdmDEQNgSI1cAf7DW/wBcOXCmAHAxsFtr3dvZwqeE1noVcCJsc7RrdAWwXGvdqrXei0nPndtfdmmtX9Ja229VXoOZuNevRLle0ei369WTbUopBXwaeLKvzh/Fpmj60Kd/Y6km6COAAyHfD5IEIqqUGgvMBN62Nn3VejxeNhChDUzZhZeUUuut+jkApdqa7GUthw6AXaFcS+f/ZAN9zSD6NUqmv7vPAy+GfB+nlHpXKfWaUuqCAbAn0r9bMl2vC4BjWuudIdv69ZqF6UOf/o2lmqDHVDOmP1FK5QBPA1/TWtdhSgdPAGYARzCPe/3NeVrrWZiyxrcqpS4cABuiosyM408Af7Y2JcM1646k+LtTSn0P8AN/tDYdAUZrrWcC3wCeUErl9aNJ0f7dkuJ6WXyGzo5Dv16zCPoQtWmEbXFfs1QT9LhrxvQlSik35h/rj1rrvwJorY9prQNa6yDwG/rwUTMaWuvD1vI48IxlwzFllTS2lsf7264QFgEbtNbHIDmumUW0azTgf3dKqZuAjwHXayvoaj2eV1vr6zFx19P7y6Zu/t0G/HoBKKVcwNXAU/a2/rxmkfSBPv4bSzVBj6WuTL9gxeZ+C2zXWv88ZHtoHfirgC3hx/axXdlKqVx7HTOgtgVznW6ymt0E/L0/7Qqjk9c00NcshGjX6FngWqWUVyk1DvMil7X9ZZRS6jJgKfAJrXVTyPYhyryABqXUeMuuPf1oV7R/twG9XiF8BNihtT5ob+ivaxZNH+jrv7G+Hu3tg9HjxZgR493A9wbQjvMxj0SbgY3WZzHwGPCetf1ZoKyf7RqPGS3fBGy1rxFQDLwC7LSWRQN03bKAaiA/ZFu/XzPMDeUI4MN4R1/o7hoB37P+5t4HFvWzXbsw8VX77+xhq+0nrX/jTcAG4OP9bFfUf7f+ul7RbLO2/x64Jaxtv1yzbvShT//GZOq/IAjCICHVQi6CIAhCFETQBUEQBgki6IIgCIMEEXRBEIRBggi6IAjCIEEEXRAEYZAggi4IgjBI+P/JsMOJL2hvAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: odin_rn_model/assets\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"test loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "odin_rn_model.save(\"odin_rn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.58%\n",
      "Test accuracy: 90.70%\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = odin_rn_model.evaluate(train_ds, verbose=0)\n",
    "_, test_acc = odin_rn_model.evaluate(test_ds, verbose=0)\n",
    "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOilU58o+F3Cx1887JA7Q+B",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "mount_file_id": "https://github.com/sayakpaul/Generalized-ODIN-TF/blob/main/Generalized_ODIN.ipynb",
   "name": "Generalized_ODIN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
